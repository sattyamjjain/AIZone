{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Object Detection with Convolution Neural Network (CNN) based on Tensorflow</h2>\n",
    "<h3>Application: Mask Detection</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will learn how to train a deep neural network for object detection in images using <a href=\"https://cs231n.github.io/transfer-learning/\">transfer learning</a> with <a href=\"https://www.tensorflow.org/lite/examples/object_detection/overview\">a Mobilenet model</a>.\n",
    "\n",
    "You will detect whether a person is wearing a mask or not. All you need to do is to annotate the provided image dataset and train the model using IBM CV Studio. In practice, very few people train an entire Convolutional Network from scratch (with random initialization) because it needs high computational power such as GPU and TPU and it is relatively rare to have a dataset of sufficient size. Instead, it is common to train a Convolutional Network on a very large dataset in the lab, then use this network to train your model. That is what we will do in this lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZSL793i7KuM"
   },
   "source": [
    "## Import Libraries and Define Auxiliary Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for OS and Cloud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-deps lvis\n",
    "!pip install tf_slim\n",
    "!pip install --no-deps tensorflowjs==1.4.0\n",
    "!pip install tensorflow==1.15.2\n",
    "!pip install tensorflow_hub\n",
    "\n",
    "from IPython.display import display, Javascript, Image\n",
    "import re\n",
    "import zipfile\n",
    "from base64 import b64decode\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "import io\n",
    "import random\n",
    "\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "import six.moves.urllib as urllib\n",
    "import PIL.Image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries for Tensorflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util, label_map_util, config_util\n",
    "from object_detection.utils.label_map_util import get_label_map_dict\n",
    "from skillsnetwork import cvstudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the files needed to build the model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"content-latest.zip\"):\n",
    "    pass\n",
    "else:\n",
    "    !wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/CV0101/content/data/content-latest.zip\n",
    "    !wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/CV0101/content/data/tfrecord.py\n",
    "\n",
    "with zipfile.ZipFile(\"content-latest.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up CV Studio Client and download the images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:27<00:00,  3.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CV Studio Client\n",
    "cvstudioClient = cvstudio.CVStudio()\n",
    "\n",
    "# Download All Images\n",
    "cvstudioClient.downloadAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the annotations from CV Studio\n",
    "annotations = cvstudioClient.get_annotations()\n",
    "labels = annotations[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the folders paths to the output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = os.path.join(os.getcwd(), \"content/checkpoint\")\n",
    "OUTPUT_PATH = os.path.join(os.getcwd(), \"content/output\")\n",
    "EXPORTED_PATH = os.path.join(os.getcwd(), \"content/exported\")\n",
    "DATA_PATH = os.path.join(os.getcwd(), \"content/data\")\n",
    "CONFIG_PATH = os.path.join(os.getcwd(), \"content/config\")\n",
    "LABEL_MAP_PATH = os.path.join(DATA_PATH, \"label_map.pbtxt\")\n",
    "TRAIN_RECORD_PATH = os.path.join(DATA_PATH, \"train.record\")\n",
    "VAL_RECORD_PATH = os.path.join(DATA_PATH, \"val.record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pre-Processing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an <code>id</code> number for each annotated image as a label map starting from 1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "with open(LABEL_MAP_PATH, \"w\") as f:\n",
    "    for idx, label in enumerate(labels):\n",
    "        f.write(\"item {\\n\")\n",
    "        f.write(\"\\tname: '{}'\\n\".format(label))\n",
    "        f.write(\"\\tid: {}\\n\".format(idx + 1))\n",
    "        f.write(\"}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tfrecord\n",
      "  Downloading tfrecord-1.14.4-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: numpy in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tfrecord) (1.21.6)\n",
      "Requirement already satisfied: protobuf in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tfrecord) (4.21.8)\n",
      "Collecting crc32c (from tfrecord)\n",
      "  Downloading crc32c-2.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: crc32c, tfrecord\n",
      "Successfully installed crc32c-2.4 tfrecord-1.14.4\n"
     ]
    }
   ],
   "source": [
    "!pip install tfrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating TFRecord is Tensorflow’s binary storage format. Using a binary file format for storage of your data can have a significant impact on the performance of your import pipeline  as a consequence on the training time of your model. Go to this link https://www.tensorflow.org/tutorials/load_data/tfrecord to learn more about it. \n",
    "\n",
    "Then, the images will be split as 70% for the training dataset <code>train_examples</code> and 30% for validation dataset <code>val_examples</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tfrecord.some_submodule'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_796/3525388276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtfrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msome_submodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_tf_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplaydetectedobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label_map_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABEL_MAP_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tfrecord.some_submodule'"
     ]
    }
   ],
   "source": [
    "from tfrecord import create_tf_record, displaydetectedobject\n",
    "\n",
    "image_files = [image for image in annotations[\"annotations\"].keys()]\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_files)\n",
    "num_train = int(0.7 * len(image_files))\n",
    "train_examples = image_files[:num_train]\n",
    "val_examples = image_files[num_train:]\n",
    "\n",
    "create_tf_record(\n",
    "    train_examples,\n",
    "    annotations[\"annotations\"],\n",
    "    label_map,\n",
    "    os.path.join(os.getcwd(), \"images\"),\n",
    "    TRAIN_RECORD_PATH,\n",
    ")\n",
    "create_tf_record(\n",
    "    val_examples,\n",
    "    annotations[\"annotations\"],\n",
    "    label_map,\n",
    "    os.path.join(os.getcwd(), \"images\"),\n",
    "    VAL_RECORD_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Model \n",
    "\n",
    "You will upload your object detection model configuration which is MobileNet V1 from tensorflow.org. There are also many other object detection models available. If you are interested go to this link (https://github.com/tensorflow/models/blob/7c2ff1afc4423266223bcd50cba0ed55aca826c8/research/object_detection/g3doc/tf1_detection_zoo.md) and you will find many other models. Training a model from scratch can take long hours and tons of data. So, we helped you in reducing that effort by training the MobileNet model with a checkpoint.\n",
    "\n",
    "The model that will be trained is the SSD MobileNet architecture. SSD MobileNet models have very small file sizes and can execute very quickly, compromising little accuracy, which makes it perfect for running in the browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oHD1Jm0v7jfz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18\"\n",
    "CONFIG_TYPE = \"ssd_mobilenet_v1_quantized_300x300_coco14_sync\"\n",
    "download_base = \"http://download.tensorflow.org/models/object_detection/\"\n",
    "model = MODEL_TYPE + \".tar.gz\"\n",
    "tmp = \"/resources/checkpoint.tar.gz\"\n",
    "\n",
    "if not (os.path.exists(CHECKPOINT_PATH)):\n",
    "    # Download the checkpoint\n",
    "    opener = urllib.request.URLopener()\n",
    "    opener.retrieve(download_base + model, tmp)\n",
    "\n",
    "    # Extract all the `model.ckpt` files.\n",
    "    with tarfile.open(tmp) as tar:\n",
    "        for member in tar.getmembers():\n",
    "            member.name = os.path.basename(member.name)\n",
    "            if \"model.ckpt\" in member.name:\n",
    "                tar.extract(member, path=CHECKPOINT_PATH)\n",
    "            if \"pipeline.config\" in member.name:\n",
    "                tar.extract(member, path=CONFIG_PATH)\n",
    "\n",
    "    os.remove(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model Training Pipeline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the the last stage before we start training the model. We need to inject our pipeline with the label map and TFRecord that we created previosuly in this notebook. As mentioned earlier, you won't start training the model from scratch, so we will use a model checkpoint. We set the batch size to be 6. This notebook won't have the sufficient memory power to handle a batch size higher than 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8CVExv6HsJS",
    "outputId": "0ad0122c-7db8-4543-9be1-f7109fad0a31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_skeleton = (\n",
    "    \"content/models/research/object_detection/samples/configs/\"\n",
    "    + CONFIG_TYPE\n",
    "    + \".config\"\n",
    ")\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_skeleton)\n",
    "\n",
    "label_map = label_map_util.get_label_map_dict(LABEL_MAP_PATH)\n",
    "num_classes = len(label_map.keys())\n",
    "meta_arch = configs[\"model\"].WhichOneof(\"model\")\n",
    "\n",
    "override_dict = {\n",
    "    \"model.{}.num_classes\".format(meta_arch): num_classes,\n",
    "    \"train_config.batch_size\": 6,\n",
    "    \"train_input_path\": TRAIN_RECORD_PATH,\n",
    "    \"eval_input_path\": VAL_RECORD_PATH,\n",
    "    \"train_config.fine_tune_checkpoint\": os.path.join(CHECKPOINT_PATH, \"model.ckpt\"),\n",
    "    \"label_map_path\": LABEL_MAP_PATH,\n",
    "}\n",
    "\n",
    "configs = config_util.merge_external_params_with_configs(\n",
    "    configs, kwargs_dict=override_dict\n",
    ")\n",
    "pipeline_config = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(pipeline_config, DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNYIZK1xVNAa"
   },
   "source": [
    "## Start Training\n",
    "We will start the training run by calling the `model_main` script. Then, we will be passing the configuration shown in the above cell as `pipepline.config`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build the model, we have to set different <code>PATH</code> to train MobileNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = [\n",
    "    f\"home/jupyterlab/conda/envs/python/lib/python3.6\",\n",
    "    f\"content/models/research\",\n",
    "    f\"content/models/research/slim\",\n",
    "]\n",
    "os.environ[\"PYTHONPATH\"] = \":\".join(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to set <code>epochs</code> as 40. That will take between 3 to 7 minutes to complete the training. One epoch is when an ENTIRE dataset is passed forward and backward through the neural network only once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "epochs = 40\n",
    "start_datetime = datetime.now()\n",
    "!python -m object_detection.model_main \\\n",
    "    --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "    --num_train_steps=$epochs \\\n",
    "    --num_eval_steps=100\n",
    "\n",
    "regex = re.compile(r\"model\\.ckpt-([0-9]+)\\.index\")\n",
    "numbers = [int(regex.search(f).group(1)) for f in os.listdir(OUTPUT_PATH) if regex.search(f)]\n",
    "TRAINED_CHECKPOINT_PREFIX = os.path.join(OUTPUT_PATH, 'model.ckpt-{}'.format(max(numbers)))\n",
    "\n",
    "!python3 -m object_detection.export_inference_graph \\\n",
    "  --pipeline_config_path=$DATA_PATH/pipeline.config \\\n",
    "  --trained_checkpoint_prefix=$TRAINED_CHECKPOINT_PREFIX \\\n",
    "  --output_directory=$EXPORTED_PATH\n",
    "end_datetime = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Test the Model and See How Cool It Is!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the image saved in this notebook. You can also add your own image by providing a URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Here you can specify your own image\n",
    "URL = \"https://cdn.cliqueinc.com/posts/289533/kamala-harris-face-mask-289533-1602269219518-square.700x0c.jpg\"\n",
    "\n",
    "with urllib.request.urlopen(URL) as url:\n",
    "    with open(\"test.jpg\", \"wb\") as f:\n",
    "        f.write(url.read())\n",
    "image = Image.open(\"test.jpg\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "n, img, accuracy = displaydetectedobject(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Report the Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\"epochs\": epochs}\n",
    "\n",
    "result = cvstudioClient.report(\n",
    "    started=start_datetime,\n",
    "    completed=end_datetime,\n",
    "    parameters=parameters,\n",
    "    accuracy=round(float(accuracy), 2) * 100,\n",
    ")\n",
    "\n",
    "if result.ok:\n",
    "    print(\"Congratulations your results have been reported back to CV Studio!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model for Deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!tensorflowjs_converter \\\n",
    "  --input_format=tf_frozen_model \\\n",
    "  --output_format=tfjs_graph_model \\\n",
    "  --output_node_names='Postprocessor/ExpandDims_1,Postprocessor/Slice' \\\n",
    "  --quantization_bytes=1 \\\n",
    "  --skip_op_check \\\n",
    "  $EXPORTED_PATH/frozen_inference_graph.pb \\\n",
    "  .\n",
    "import json\n",
    "\n",
    "from object_detection.utils.label_map_util import get_label_map_dict\n",
    "\n",
    "label_map = get_label_map_dict(LABEL_MAP_PATH)\n",
    "label_array = [k for k in sorted(label_map, key=label_map.get)]\n",
    "\n",
    "with open(os.path.join('', 'labels.json'), 'w') as f:\n",
    "    json.dump(label_array, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd model_web\n",
    "with ZipFile(\"model_web.zip\", \"w\") as zip:\n",
    "    zip.write(\"group1-shard1of2.bin\")\n",
    "    zip.write(\"group1-shard2of2.bin\")\n",
    "    zip.write(\"model.json\")\n",
    "    zip.write(\"labels.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download All Images\n",
    "cvstudioClient.uploadModel(\"model_web.zip\", {\"epochs\": epochs})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " object-detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
